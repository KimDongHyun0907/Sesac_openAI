### 웹 크롤링 (Web Crawling)
- 웹 사이트를 자동으로 돌아다니며 웹페이지의 링크를 따라가며 데이터를 수집하는 과정
- 크롤러 또는 스파이더로 여러 웹 페이지를 탐색하고 필요한 정보 추출
- 검색 엔진이 웹의 새로운 내용을 수집, 인덱싱

### 웹 스크래핑 (Web Scraping)
- 특정 웹 페이지에서 데이터를 추출하는 프로세스를 의미
- 웹 페이지의 HTML을 분석하고 필요한 정보 추출.
- 정규 표현식이나 파싱 라이브리(BeautifulSoup)을 사용

### 차이점
- 웹 크롤링: 여러 웹 페이지에서 링크를 따라가며 정보 수집
- 웹 스크래핑 : 특정 웹페이지에서 필요한 데이터 추출

### robot.txt
- 웹 크롤러나 스파이더에게 특정 웹사이트의 크롤링 규칙을 알려주기 위한 파일
- 정보를 수집할 수 있는 부분, 수집할 수 없는 부분을 제공
- **구조**
  - User-agent : 크롤러의 식별자. 여러 크롤러에 대해 각각 따로 규칙 지정
  - Disallow : 크롤링하지 말아야 하는 경로
  - Allow : 크롤링해도 되는 경로


## 서버 통신
### HTTP
- HTTP는 클라이언트와 서버 사이에 이루어지는 요청/응답 프로토콜.
- 클라이언트가 브라우저를 통해 서비스 URL을 통하거나 다른 방법으로 요청 (request)
- 서버에서는 해당 요청사항에 맞는 결과를 찾아 사용자에게 응답 (response)

### Request (요청)
- 클라이언트가 서버에게 연락하는 것
- 요청을 보낼 때 요청에 대한 정보를 담아 서버로 보냄.
- Request Method
  - GET : 리소스를 검색. 반환받기 위해 사용.
  - HEAD : 서버의 각종 정보 확인
  - POST : 요청된 자원 생성
  - PUT : 요청된 자원 전체 수정
  - PATCH : 요청된 자원의 일부 수정
  - DELETE : 요청된 자원 삭제. 일반적으로 delete flag를 사용하여 수정하는 방법 사용.
  - TRACE : 루프백 메시지를 호출
  - OPTION : 웹서버에서 지원하는 메소드를 알기 위한 명령어
  - CONNECT : 프록시 기능 요청

### Response (응답)
- 서버가 요청에 대한 답변을 클라이언트에게 보내는 것.
- **Status Code**
  - 1xx (조건부 응답) : 요청을 받았으며 작업을 계속한다.
  - 2xx (성공) : 클라이언트가 요청한 동작을 수신. 성공적으로 처리
  - 3xx (리다이렉션 완료) : 클라이언트는 요청을 마치기 위해 추가 동작 처리
  - 4xx (요청 오류) : 클라이언트에 오류가 있음.
  - 5xx (서버 오류) : 서버가 유효한 요청을 수행하지 못했음.

**클라이언트와 서버 간의 통신**  
![image](https://github.com/KimDongHyun0907/Sesac_openAI/assets/88826811/512b14f1-0b3b-4089-ae4c-f3815d87d4bf)  

### requests 라이브러리
- 파이썬에서 HTTP 요청을 보내는 방법
- 장점
  - 간편한 사용법 : 사용하기 쉬운 API를 제공하여 누구나 쉽게 HTTP 요청을 보낼 수 있음
  - 다양한 기능 : 다양한 HTTP 메서드를 지원. 세션 관리, 쿠키 처리, 인증 등
- 단점
  - 성능 : GIL (Global Interpreter Lock)로 인해 멀티스레드 환경에서 성능이 저하
  - 속도 : urllib3를 사용하여 HTTP 요청 처리. C로 구현되어 속도가 빠름. 비동기적인 요청을 처리하는 라이브러리들과 비교하면 상대적으로 느림

### BeautifulSoup 메서드
1. find, find_all
   - 특정 태그를 가진 요소를 찾기
2. select
   - CSS 선택자를 사용하여 원하는 요소 찾기

