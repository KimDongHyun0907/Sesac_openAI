{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터 전처리\n",
    "- 비속어와 비속어가 아닌 문장을 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>여성/가족</th>\n",
       "      <th>남성</th>\n",
       "      <th>성소수자</th>\n",
       "      <th>인종/국적</th>\n",
       "      <th>연령</th>\n",
       "      <th>지역</th>\n",
       "      <th>종교</th>\n",
       "      <th>기타 혐오</th>\n",
       "      <th>악플/욕설</th>\n",
       "      <th>clean</th>\n",
       "      <th>개인지칭</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>일안하는 시간은 쉬고싶어서 그런게 아닐까</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>께롱께롱께롱!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>갱상도가아니라 홍어지 개좃같은 홍어년들</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>말레이시아랑 인도네시아 여자 존나 못생겼던데</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>링크도 안박고 가서 글 쓰자고 선동하네</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>멸치가 틀딱 짜증나서 트러블나면 조직적으로 좀스럽게 보복함ex: 수건찜유도탄, 틀한...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15005 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  여성/가족  남성  성소수자  \\\n",
       "0                                 일안하는 시간은 쉬고싶어서 그런게 아닐까      0   0     0   \n",
       "1      아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...      0   0     0   \n",
       "2      루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...      0   0     0   \n",
       "3      홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...      0   0     0   \n",
       "4      아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...      1   0     0   \n",
       "...                                                  ...    ...  ..   ...   \n",
       "15000                                          께롱께롱께롱!!!      0   0     0   \n",
       "15001                              갱상도가아니라 홍어지 개좃같은 홍어년들      0   0     0   \n",
       "15002                           말레이시아랑 인도네시아 여자 존나 못생겼던데      1   0     0   \n",
       "15003                              링크도 안박고 가서 글 쓰자고 선동하네      0   0     0   \n",
       "15004  멸치가 틀딱 짜증나서 트러블나면 조직적으로 좀스럽게 보복함ex: 수건찜유도탄, 틀한...      0   0     0   \n",
       "\n",
       "       인종/국적  연령  지역  종교  기타 혐오  악플/욕설  clean  개인지칭  \n",
       "0          0   0   0   0      0      0      1     0  \n",
       "1          0   0   0   1      0      0      0     0  \n",
       "2          0   0   0   0      0      0      1     0  \n",
       "3          0   0   0   0      0      0      1     0  \n",
       "4          0   0   0   0      0      0      0     0  \n",
       "...      ...  ..  ..  ..    ...    ...    ...   ...  \n",
       "15000      0   0   0   0      0      0      1     0  \n",
       "15001      0   0   1   0      0      0      0     0  \n",
       "15002      1   0   0   0      0      0      0     0  \n",
       "15003      0   0   0   0      0      1      0     0  \n",
       "15004      0   1   0   0      0      0      0     0  \n",
       "\n",
       "[15005 rows x 12 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# TSV 파일 경로\n",
    "tsv_file_path = 'korean_unsmile_dataset/unsmile_train_v1.0.tsv'\n",
    "\n",
    "# TSV 파일 열기\n",
    "df = pd.read_csv(tsv_file_path, sep='\\t')\n",
    "\n",
    "# 데이터프레임 출력\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.concat([df['문장'], df['clean']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>문장</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>일안하는 시간은 쉬고싶어서 그런게 아닐까</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15000</th>\n",
       "      <td>께롱께롱께롱!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15001</th>\n",
       "      <td>갱상도가아니라 홍어지 개좃같은 홍어년들</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15002</th>\n",
       "      <td>말레이시아랑 인도네시아 여자 존나 못생겼던데</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15003</th>\n",
       "      <td>링크도 안박고 가서 글 쓰자고 선동하네</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15004</th>\n",
       "      <td>멸치가 틀딱 짜증나서 트러블나면 조직적으로 좀스럽게 보복함ex: 수건찜유도탄, 틀한...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>15005 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      문장  clean\n",
       "0                                 일안하는 시간은 쉬고싶어서 그런게 아닐까      1\n",
       "1      아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...      0\n",
       "2      루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...      1\n",
       "3      홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...      1\n",
       "4      아놔 왜 여기 댓들은 다 여자들이 김치녀라고 먼저 불렸다! 여자들은 더 심하게 그런...      0\n",
       "...                                                  ...    ...\n",
       "15000                                          께롱께롱께롱!!!      1\n",
       "15001                              갱상도가아니라 홍어지 개좃같은 홍어년들      0\n",
       "15002                           말레이시아랑 인도네시아 여자 존나 못생겼던데      0\n",
       "15003                              링크도 안박고 가서 글 쓰자고 선동하네      0\n",
       "15004  멸치가 틀딱 짜증나서 트러블나면 조직적으로 좀스럽게 보복함ex: 수건찜유도탄, 틀한...      0\n",
       "\n",
       "[15005 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('clean_or_dirty_words.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained BERT 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
      "     ---------------------------------------- 0.0/134.8 kB ? eta -:--:--\n",
      "     -------- ------------------------------ 30.7/134.8 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------  133.1/134.8 kB 2.0 MB/s eta 0:00:01\n",
      "     -------------------------------------- 134.8/134.8 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers)\n",
      "  Downloading huggingface_hub-0.22.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\bluecom014\\miniconda3\\envs\\final\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\bluecom014\\miniconda3\\envs\\final\\lib\\site-packages (from transformers) (24.0)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/42.0 kB ? eta -:--:--\n",
      "     ---------------------------------------- 42.0/42.0 kB ? eta 0:00:00\n",
      "Collecting requests (from transformers)\n",
      "  Using cached requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers)\n",
      "  Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.2-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "     ---------------------------------------- 0.0/57.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 57.6/57.6 kB ? eta 0:00:00\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.19.3->transformers)\n",
      "  Downloading fsspec-2024.3.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\bluecom014\\miniconda3\\envs\\final\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\bluecom014\\miniconda3\\envs\\final\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->transformers)\n",
      "  Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->transformers)\n",
      "  Using cached idna-3.6-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->transformers)\n",
      "  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->transformers)\n",
      "  Downloading certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Downloading transformers-4.39.2-py3-none-any.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.5/8.8 MB 10.5 MB/s eta 0:00:01\n",
      "   ---- ----------------------------------- 1.0/8.8 MB 11.1 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 1.5/8.8 MB 10.5 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.2/8.8 MB 12.7 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.8/8.8 MB 13.0 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.5/8.8 MB 14.1 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.7/8.8 MB 15.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 5.5/8.8 MB 16.0 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.4/8.8 MB 17.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/8.8 MB 17.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.4/8.8 MB 18.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/8.8 MB 18.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 8.8/8.8 MB 17.1 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.22.1-py3-none-any.whl (388 kB)\n",
      "   ---------------------------------------- 0.0/388.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 388.6/388.6 kB ? eta 0:00:00\n",
      "Downloading PyYAML-6.0.1-cp311-cp311-win_amd64.whl (144 kB)\n",
      "   ---------------------------------------- 0.0/144.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 144.7/144.7 kB ? eta 0:00:00\n",
      "Downloading regex-2023.12.25-cp311-cp311-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 269.5/269.5 kB ? eta 0:00:00\n",
      "Downloading safetensors-0.4.2-cp311-none-win_amd64.whl (269 kB)\n",
      "   ---------------------------------------- 0.0/269.6 kB ? eta -:--:--\n",
      "   --------------------------------------- 269.6/269.6 kB 16.2 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.15.2-cp311-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   --------------------- ------------------ 1.2/2.2 MB 37.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 28.0 MB/s eta 0:00:00\n",
      "Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "   ---------------------------------------- 0.0/78.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 78.3/78.3 kB ? eta 0:00:00\n",
      "Downloading filelock-3.13.3-py3-none-any.whl (11 kB)\n",
      "Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Downloading certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "   ---------------------------------------- 0.0/163.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 163.8/163.8 kB 9.6 MB/s eta 0:00:00\n",
      "Downloading charset_normalizer-3.3.2-cp311-cp311-win_amd64.whl (99 kB)\n",
      "   ---------------------------------------- 0.0/99.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 99.9/99.9 kB ? eta 0:00:00\n",
      "Downloading fsspec-2024.3.1-py3-none-any.whl (171 kB)\n",
      "   ---------------------------------------- 0.0/172.0 kB ? eta -:--:--\n",
      "   --------------------------------------- 172.0/172.0 kB 10.1 MB/s eta 0:00:00\n",
      "Using cached idna-3.6-py3-none-any.whl (61 kB)\n",
      "Downloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n",
      "   ---------------------------------------- 0.0/121.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 121.1/121.1 kB ? eta 0:00:00\n",
      "Installing collected packages: urllib3, tqdm, safetensors, regex, pyyaml, idna, fsspec, filelock, charset-normalizer, certifi, requests, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed certifi-2024.2.2 charset-normalizer-3.3.2 filelock-3.13.3 fsspec-2024.3.1 huggingface-hub-0.22.1 idna-3.6 pyyaml-6.0.1 regex-2023.12.25 requests-2.31.0 safetensors-0.4.2 tokenizers-0.15.2 tqdm-4.66.2 transformers-4.39.2 urllib3-2.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bluecom014\\miniconda3\\envs\\final\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\bluecom014\\miniconda3\\envs\\final\\Lib\\site-packages\\transformers\\pipelines\\text_classification.py:104: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextClassificationPipeline, BertForSequenceClassification, AutoTokenizer\n",
    "model_name = 'sgunderscore/hatescore-korean-hate-speech'\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "pipe = TextClassificationPipeline(\n",
    "        model = model,\n",
    "        tokenizer = tokenizer,\n",
    "        device = -1, # gpu: 0\n",
    "        return_all_scores = True,\n",
    "        function_to_apply = 'sigmoid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정상 댓글\n"
     ]
    }
   ],
   "source": [
    "#text = \"야 이 병신같은 새끼야. 대가리를 왜 달고 다님?\"\n",
    "text = '안녕하세요'\n",
    "label_and_score = pipe(text)[0]\n",
    "\n",
    "label_and_score = sorted(label_and_score, key=lambda x:-x['score'])\n",
    "\n",
    "if label_and_score[0]['label'] == 'None':\n",
    "  print('정상 댓글')\n",
    "\n",
    "else:\n",
    "  print('비속어 감지')\n",
    "  print(f'{label_and_score[0][\"label\"]} 비하 글이므로 필터링 필요.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretrained 모델 정확도 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15005 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15005/15005 [16:54<00:00, 14.79it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.852315894701766"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "labels = df2['clean'].tolist()\n",
    "predictions = []\n",
    "for data in tqdm(df2['문장'].tolist()):\n",
    "    result = pipe(data)[0]\n",
    "    result = sorted(result, key=lambda x:-x['score'])\n",
    "    predict_label = 1 if result[0]['label'] == 'None' else 0\n",
    "    predictions.append(predict_label)\n",
    "\n",
    "accuracy = accuracy_score(labels, predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 비속어, 민감정보 둘 다 없음  false, false  \n",
    "1 비속어만 있음  true false  \n",
    "2 민감정보만 있음  false true  \n",
    "3 비속어, 민감정보 둘 다 있음  true true  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# sample_text = \"\"\"\n",
    "# 개인 정보 보호를 위해 전화번호 010-1234-5678와 이메일 test@example.com,\n",
    "# 그리고 주민등록번호 920101-1234567을 마스킹합니다.\n",
    "# \"\"\"\n",
    "\n",
    "sample_text = \"010-1234-5678 이 번호로 연락오면 받아라. 안 받으면 죽여버린다 개새끼야.\"\n",
    "\n",
    "def pattern_match(text):\n",
    "    email_pattern = r'\\b([A-Za-z0-9._%+-]{2})([A-Za-z0-9._%+-]+)@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    phone_pattern = r'(\\d{3})-(\\d{4})-(\\d{4})'\n",
    "    resident_id_pattern = r'(\\d{6})-?([1-4])(\\d{6})'\n",
    "\n",
    "    matches_email = re.findall(email_pattern, text)\n",
    "    matches_phone = re.findall(phone_pattern, text)\n",
    "    matches_id = re.findall(resident_id_pattern, text)\n",
    "    \n",
    "    return matches_email or matches_phone or matches_id\n",
    "\n",
    "# if pattern_match(sample_text):\n",
    "#     print(\"패턴 적용\")\n",
    "# else:\n",
    "#     print(\"패턴 적용 x\")\n",
    "\n",
    "def bad_words_find(text):\n",
    "    label_and_score = pipe(text)[0]\n",
    "    label_and_score = sorted(label_and_score, key=lambda x:-x['score'])\n",
    "    return False if label_and_score[0]['label'] == 'None' else True\n",
    "\n",
    "# bad_words_find(sample_text)\n",
    "\n",
    "if bad_words_find(sample_text):\n",
    "    label = 3 if pattern_match(sample_text) else 1\n",
    "else:\n",
    "    label = 2 if pattern_match(sample_text) else 0\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㄱ',\n",
       " 'ㄲ',\n",
       " 'ㄳ',\n",
       " 'ㄴ',\n",
       " 'ㄵ',\n",
       " 'ㄶ',\n",
       " 'ㄷ',\n",
       " 'ㄸ',\n",
       " 'ㄹ',\n",
       " 'ㄺ',\n",
       " 'ㄻ',\n",
       " 'ㄼ',\n",
       " 'ㄽ',\n",
       " 'ㄾ',\n",
       " 'ㄿ',\n",
       " 'ㅀ',\n",
       " 'ㅁ',\n",
       " 'ㅂ',\n",
       " 'ㅃ',\n",
       " 'ㅄ',\n",
       " 'ㅅ',\n",
       " 'ㅆ',\n",
       " 'ㅇ',\n",
       " 'ㅈ',\n",
       " 'ㅉ',\n",
       " 'ㅊ',\n",
       " 'ㅋ',\n",
       " 'ㅌ',\n",
       " 'ㅍ',\n",
       " 'ㅎ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[chr(i) for i in range(ord('ㄱ'), ord('ㅎ')+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ㅊ', 'ㅟ', ' '],\n",
       " ['ㅃ', 'ㅗ', ' '],\n",
       " ['ㅎ', 'ㅏ', ' '],\n",
       " ['ㄱ', 'ㅗ', ' '],\n",
       " [' '],\n",
       " ['ㅅ', 'ㅣ', 'ㅍ'],\n",
       " ['ㄷ', 'ㅏ', ' '],\n",
       " ['.'],\n",
       " [' '],\n",
       " ['ㄴ', 'ㅏ', ' '],\n",
       " [' '],\n",
       " ['ㅈ', 'ㅗ', 'ㅁ'],\n",
       " [' '],\n",
       " ['ㄷ', 'ㅔ', ' '],\n",
       " ['ㄹ', 'ㅕ', ' '],\n",
       " ['ㄱ', 'ㅏ', ' '],\n",
       " ['ㅈ', 'ㅝ', ' '],\n",
       " ['~'],\n",
       " ['~']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://frhyme.github.io/python/python_korean_englished/\n",
    "# 문장을 자음, 모음 나누기\n",
    "\n",
    "# 초성 리스트. 00 ~ 18\n",
    "CHOSUNG_LIST = ['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "# 중성 리스트. 00 ~ 20\n",
    "JUNGSUNG_LIST = ['ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ']\n",
    "# 종성 리스트. 00 ~ 27 + 1(1개 없음)\n",
    "JONGSUNG_LIST = [' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ']\n",
    "\n",
    "def korean_to_be_englished(korean_word):\n",
    "    r_lst = []\n",
    "    for w in list(korean_word.strip()):\n",
    "        ## 영어인 경우 구분해서 작성함. \n",
    "        if '가'<=w<='힣':\n",
    "            ## 588개 마다 초성이 바뀜. \n",
    "            ch1 = (ord(w) - ord('가'))//588\n",
    "            ## 중성은 총 28가지 종류\n",
    "            ch2 = ((ord(w) - ord('가')) - (588*ch1)) // 28\n",
    "            ch3 = (ord(w) - ord('가')) - (588*ch1) - 28*ch2\n",
    "            r_lst.append([CHOSUNG_LIST[ch1], JUNGSUNG_LIST[ch2], JONGSUNG_LIST[ch3]])\n",
    "        else:\n",
    "            r_lst.append([w])\n",
    "    return r_lst\n",
    "    \n",
    "korean_to_be_englished(\"취뽀하고 싶다. 나 좀 데려가줘~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scienceon.kisti.re.kr/commons/util/originalView.do?cn=CFKO201924664108493&oCn=NPAP13263940&dbt=CFKO&journal=NPRO00383455  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ㄱ', 'ㄲ', 'ㄴ', 'ㄷ', 'ㄸ', 'ㄹ', 'ㅁ', 'ㅂ', 'ㅃ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅉ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', 'ㅏ', 'ㅐ', 'ㅑ', 'ㅒ', 'ㅓ', 'ㅔ', 'ㅕ', 'ㅖ', 'ㅗ', 'ㅘ', 'ㅙ', 'ㅚ', 'ㅛ', 'ㅜ', 'ㅝ', 'ㅞ', 'ㅟ', 'ㅠ', 'ㅡ', 'ㅢ', 'ㅣ', ' ', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', 'ㄵ', 'ㄶ', 'ㄷ', 'ㄹ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅁ', 'ㅂ', 'ㅄ', 'ㅅ', 'ㅆ', 'ㅇ', 'ㅈ', 'ㅊ', 'ㅋ', 'ㅌ', 'ㅍ', 'ㅎ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '!', '\"', '#', '$', '%', '&', \"'\", '?', '@', '*', '+', ',', '-', '.', '/', '~', ':', '^', ' ', 'ㄳ', 'ㄵ', 'ㅄ', 'ㄺ']\n"
     ]
    }
   ],
   "source": [
    "# 자음, 모음 카데고리 사전 만들기\n",
    "NUMBER_LIST = list(map(str, [i for i in range(10)]))\n",
    "ALPHABET_LIST = [chr(i) for i in range(ord('a'), ord('z')+1)]\n",
    "SPECIAL_WORD_LIST = ['!', '\"', '#', '$', '%', '&', '\\'', '?','@', '*', '+',',','-','.', '/', '~', ':', '^', ' ']\n",
    "ONEWORD_LIST = ['ㄳ', 'ㄵ', 'ㅄ', 'ㄺ']\n",
    "\n",
    "word_vector_list = CHOSUNG_LIST+JUNGSUNG_LIST+JONGSUNG_LIST+NUMBER_LIST+ALPHABET_LIST+SPECIAL_WORD_LIST+ONEWORD_LIST\n",
    "print(word_vector_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ㄱ': [0, 41], 'ㄲ': [1, 42], 'ㄴ': [2, 44], 'ㄷ': [3, 47], 'ㄸ': [4], 'ㄹ': [5, 48], 'ㅁ': [6, 56], 'ㅂ': [7, 57], 'ㅃ': [8], 'ㅅ': [9, 59], 'ㅆ': [10, 60], 'ㅇ': [11, 61], 'ㅈ': [12, 62], 'ㅉ': [13], 'ㅊ': [14, 63], 'ㅋ': [15, 64], 'ㅌ': [16, 65], 'ㅍ': [17, 66], 'ㅎ': [18, 67], 'ㅏ': [19], 'ㅐ': [20], 'ㅑ': [21], 'ㅒ': [22], 'ㅓ': [23], 'ㅔ': [24], 'ㅕ': [25], 'ㅖ': [26], 'ㅗ': [27], 'ㅘ': [28], 'ㅙ': [29], 'ㅚ': [30], 'ㅛ': [31], 'ㅜ': [32], 'ㅝ': [33], 'ㅞ': [34], 'ㅟ': [35], 'ㅠ': [36], 'ㅡ': [37], 'ㅢ': [38], 'ㅣ': [39], ' ': [40, 122], 'ㄳ': [43, 123], 'ㄵ': [45, 124], 'ㄶ': [46], 'ㄺ': [49, 126], 'ㄻ': [50], 'ㄼ': [51], 'ㄽ': [52], 'ㄾ': [53], 'ㄿ': [54], 'ㅀ': [55], 'ㅄ': [58, 125], '0': [68], '1': [69], '2': [70], '3': [71], '4': [72], '5': [73], '6': [74], '7': [75], '8': [76], '9': [77], 'a': [78], 'b': [79], 'c': [80], 'd': [81], 'e': [82], 'f': [83], 'g': [84], 'h': [85], 'i': [86], 'j': [87], 'k': [88], 'l': [89], 'm': [90], 'n': [91], 'o': [92], 'p': [93], 'q': [94], 'r': [95], 's': [96], 't': [97], 'u': [98], 'v': [99], 'w': [100], 'x': [101], 'y': [102], 'z': [103], '!': [104], '\"': [105], '#': [106], '$': [107], '%': [108], '&': [109], \"'\": [110], '?': [111], '@': [112], '*': [113], '+': [114], ',': [115], '-': [116], '.': [117], '/': [118], '~': [119], ':': [120], '^': [121]}\n"
     ]
    }
   ],
   "source": [
    "# 카데고리 사전의 각 데이터와 인덱스를 딕셔너리로 추출\n",
    "# 자음일 때 예를 들어 'ㄱ': [0, 41]은 ㄱ이 초성으로 오면 0, 종성으로 오면 41\n",
    "# ' '(공백) 이면 ' ': [40, 122]. 40은 글자에 종성이 없을 때. 122는 단어와 단어 사이 띄어쓰기\n",
    "\n",
    "word_vector_idx_dict = {}\n",
    "\n",
    "for idx, data in enumerate(word_vector_list):\n",
    "    if not data in word_vector_idx_dict:\n",
    "        word_vector_idx_dict[data] = [] \n",
    "    word_vector_idx_dict[data].append(idx)\n",
    "print(word_vector_idx_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ㄳ', 'ㄵ', 'ㄶ', 'ㄺ', 'ㄻ', 'ㄼ', 'ㄽ', 'ㄾ', 'ㄿ', 'ㅀ', 'ㅄ']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 자음 중 초성과 종성에서 종성에만 있는 데이터를 따로 추출\n",
    "\n",
    "JONGSUNG_ONLY = sorted(list(set(JONGSUNG_LIST)-set(CHOSUNG_LIST)))[1:]\n",
    "JONGSUNG_ONLY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력한 문장의 자음, 모음 벡터화\n",
    "\n",
    "def make_input_vector(text):\n",
    "    input_vector = [[0 for _ in range(127)] for _ in range(200)]\n",
    "    text = text.lower()  # 영어는 소문자로 바꿈\n",
    "\n",
    "    consonants_and_vowels = korean_to_be_englished(text)\n",
    "\n",
    "    if len(sum(consonants_and_vowels,[]))<200:  # 입력 문자의 벡터는 200자 이내로 제한\n",
    "        cnt = 0  # input_vector의 index\n",
    "        for words in consonants_and_vowels:\n",
    "            if len(words) == 3:  # 한글일 때 (초성, 중성, 종성)\n",
    "                for idx in range(3):\n",
    "                    if idx == 0 or idx == 1:  # 초성, 중성\n",
    "                        input_vector[cnt][word_vector_idx_dict[words[idx]][0]]=1\n",
    "                    \n",
    "                    else:  # 종성\n",
    "                        if words[idx] == ' ' or words[idx] in JONGSUNG_ONLY:  # 종성이 없을 때, 자음 중 종성만 있을 때\n",
    "                            input_vector[cnt][word_vector_idx_dict[words[idx]][0]]=1\n",
    "                        else:  # 자음 중 초성과 종성 둘 다 있는 경우, 종성\n",
    "                            input_vector[cnt][word_vector_idx_dict[words[idx]][1]]=1\n",
    "                    cnt += 1\n",
    "                    \n",
    "            else:  # 한글이 아닌 경우 (특수문자, 영어)\n",
    "                if words[0] not in word_vector_list:  # word_vector_list에 없는 특수문자는 제외\n",
    "                    pass\n",
    "                elif words[0] == ' ':  # 띄어쓰기인 경우\n",
    "                    input_vector[cnt][word_vector_idx_dict[words[0]][1]]=1\n",
    "                else:  # 특수문자, 영어인 경우\n",
    "                    input_vector[cnt][word_vector_idx_dict[words[0]][0]]=1\n",
    "                cnt += 1\n",
    "\n",
    "        return input_vector\n",
    "    \n",
    "    else:\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tsv의 문장들을 one hot vector로 변환\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "pickle_data = pd.DataFrame(columns=['sentence', 'onehot_vector', 'clean'])\n",
    "for data in df2['문장']:\n",
    "    data_vector = np.array(make_input_vector(data))\n",
    "    if data_vector.shape == (200, 127):\n",
    "        target = df2.loc[df2['문장']==data].values[0][1]\n",
    "        pickle_data = pd.concat([pickle_data, pd.DataFrame({'sentence':[data], 'onehot_vector':[data_vector], 'clean':[target]})], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>onehot_vector</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>일안하는 시간은 쉬고싶어서 그런게 아닐까</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>예수 십새끼 개새끼 창녀아들 애비실종 가정교육 못받은 무뇌충 허언증 정신병자 사기꾼...</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13433</th>\n",
       "      <td>성소수자 이빈 이띤 좆같고 뱡신 같은 말은 누가 만드는거냐?</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13434</th>\n",
       "      <td>께롱께롱께롱!!!</td>\n",
       "      <td>[[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13435</th>\n",
       "      <td>갱상도가아니라 홍어지 개좃같은 홍어년들</td>\n",
       "      <td>[[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13436</th>\n",
       "      <td>말레이시아랑 인도네시아 여자 존나 못생겼던데</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13437</th>\n",
       "      <td>링크도 안박고 가서 글 쓰자고 선동하네</td>\n",
       "      <td>[[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13438 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                sentence  \\\n",
       "0                                 일안하는 시간은 쉬고싶어서 그런게 아닐까   \n",
       "1      아동성범죄와 페도버는 기록바 끊어져 영원히 고통 받는다. 무슬림 50퍼 근친이다. ...   \n",
       "2      루나 솔로앨범 나왔을 때부터 머모 기운 있었음 ㅇㅇ Keep o  doin 진짜 띵...   \n",
       "3      홍팍에도 어버이연합인가 보내요 뭐 이런뎃글 있는데 이거 어버이연합측에 신고하면 그쪽...   \n",
       "4      예수 십새끼 개새끼 창녀아들 애비실종 가정교육 못받은 무뇌충 허언증 정신병자 사기꾼...   \n",
       "...                                                  ...   \n",
       "13433                  성소수자 이빈 이띤 좆같고 뱡신 같은 말은 누가 만드는거냐?   \n",
       "13434                                          께롱께롱께롱!!!   \n",
       "13435                              갱상도가아니라 홍어지 개좃같은 홍어년들   \n",
       "13436                           말레이시아랑 인도네시아 여자 존나 못생겼던데   \n",
       "13437                              링크도 안박고 가서 글 쓰자고 선동하네   \n",
       "\n",
       "                                           onehot_vector clean  \n",
       "0      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...     1  \n",
       "1      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...     0  \n",
       "2      [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     1  \n",
       "3      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     1  \n",
       "4      [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...     0  \n",
       "...                                                  ...   ...  \n",
       "13433  [[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,...     0  \n",
       "13434  [[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     1  \n",
       "13435  [[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     0  \n",
       "13436  [[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,...     0  \n",
       "13437  [[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,...     0  \n",
       "\n",
       "[13438 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickle_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일로 저장 (용량이 커서 gzip 사용)\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('hs.pickle', 'wb') as f:\n",
    "    pickle.dump(pickle_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle 파일 읽기\n",
    "import pickle\n",
    "import gzip\n",
    "\n",
    "with gzip.open('hs.pickle', 'rb') as f:\n",
    "    data = pickle.load(f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sesac",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
